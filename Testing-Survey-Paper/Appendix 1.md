# Definition Provided in the Survey
We refer to Figure 1 for the survey questions.
In this section we listed the definitions we provided in the actual survey.
* **Developer** - An individual who writes, debugs, and executes the source code of a software application.
* **Architect** - An individual who is a software development expert who makes high-level design choices and dictates technical standards, including software coding standards, tools, and platforms.
* **Quality Assurance Engineer** - An individual who tracks the development process, oversee production, testing each part to ensure it meets standards before moving to the next phase.
* **Maintainer** - An individual who builds source code into a binary package for distribution, commit patches or organize code in a source repository.
* **Manager** - An individual who is responsible for overseeing and coordinating the people, resources, and processes required to deliver new software or upgrade existing products.
* **Executive** - An individual who establishes and directs the strategic long term goals, policies, and procedures for an organization's software development program.
* **Unit testing** - Asses software with respect to implementation.
* **Integration testing** - Asses software with respect to subsystem design.
* **System testing** - Asses software with respect to architectural design and overall behavior.
* **Acceptance testing** - Assess software with respect to requirements or usersâ€™ needs.
* **Module testing** - Asses software with respect to detailed design.
* **Metamorphic testing** - Testing how a particular change in input of the program would change the output.
* **Assertion checking** - Testing some necessary property of the program under test using a boolean expression or a constraint to verify.
* **Performance testing** - Testing some of the non-functional quality attributes of software like Stability, reliability, availability.
* **Monte carlo test** - Testing numerical results using repeated random sampling.
* **Dual coding** - Testing the models created using two different algorithms while using the same or most common set of features.
* **Fuzzing test** - Testing the software for failures or error messages that are presented due to unexpected or random inputs.
* **Backward compatibility testing** - Testing whether the newly updated software works well with an older version of the environment or not.
* **Using machine learning** - Testing the output values using different machine learning techniques.
* **Using statistical tests** - Testing the output values using different statistical tests.
* **Test driven development** - Testing the output by writing an (initially failing) automated test case that defines a desired improvement or new function, then produces the minimum amount of code to pass that test.
* **Input space partitioning** - Testing the output by dividing the input space according to logical partitioning and choosing elements from the input space of the software being tested.
* **Graph coverage** - Testing code coverage by mapping executable statements and branches to a control flow graph and cover the graph in some way.
* **Logic coverage** - Testing both semantic and syntactic meaning of how a logical expression is formulated.
* **Statement coverage** - Testing code coverage by making sure all statements in the program source code are tested at least once.
* **Condition coverage** - Testing code coverage by making sure all conditions in the program source code are tested at least once.
* **Branch coverage** - Testing code coverage by making sure all branches in the program source code are tested at least once.
* **Syntax-based testing** - Testing the output using syntax to generate artifacts that are valid or invalid.
* **Boundary value analysis** - Testing the output by checking if defects exist at boundary values.
* **Equivalence partitioning** - Testing a set of the group by picking a few values or numbers to understood that all values from that group generate the same output.
* **Decision table based testing** - Testing the output by dealing with different combinations of inputs which produce different results.
* **State transition** -  Testing the outputs by changes to the input conditions or changes to 'state' of the system.
* **Error Guessing** - Testing the output where the test analyst uses his / her experience to guess the problematic areas of the application.
* **Backward compatibility testing** - Testing whether the newly updated software works well with an older version of the environment or not.